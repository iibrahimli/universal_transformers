{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import UniversalTransformer, PositionalTimestepEncoding\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the first line if you are not on an M1 Mac\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"mps\")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "D_MODEL = 64\n",
    "MAX_LEN = 50\n",
    "DROPOUT = 0.1\n",
    "\n",
    "pos_enc = PositionalTimestepEncoding(D_MODEL, DROPOUT, MAX_LEN)\n",
    "\n",
    "x = torch.randn(BATCH_SIZE, MAX_LEN, D_MODEL)\n",
    "pos_enc.pe.shape, pos_enc(x, time_step=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_enc.pe.shape)\n",
    "plt.imshow(pos_enc.pe.squeeze().detach())\n",
    "plt.xlabel(r\"$d_{model}$\")\n",
    "plt.ylabel(\"Sequence length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = torch.randint(0, 50_000, (BATCH_SIZE, MAX_LEN))\n",
    "tgt_mask = UniversalTransformer.generate_subsequent_mask(tgt)\n",
    "\n",
    "print(tgt.shape, tgt_mask.shape)\n",
    "plt.imshow(tgt_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check: overfitting on a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    source_vocab_size=100,\n",
    "    target_vocab_size=100,\n",
    "    d_model=32,\n",
    "    n_head=8,\n",
    "    d_feedforward=64,\n",
    "    max_len=MAX_LEN,\n",
    "    max_time_step=20,\n",
    "    halting_thresh=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UniversalTransformer(**config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Number of trainable parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.randint(0, 100, (BATCH_SIZE, MAX_LEN)).to(DEVICE)\n",
    "tgt = torch.randint(0, 100, (BATCH_SIZE, MAX_LEN // 2)).to(DEVICE)\n",
    "# tgt_mask = UniversalTransformer.generate_subsequent_mask(tgt)\n",
    "out = model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(label_smoothing=0.1).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = copy.deepcopy(model)\n",
    "out = model(src, tgt)\n",
    "max_val, max_id = out.max(dim=-1)\n",
    "torch.isclose(max_id, tgt).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb stuff\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"universal_transformer_overfit_test\", config=config\n",
    ")\n",
    "wandb.watch(model, log_freq=100)\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(src, tgt)\n",
    "    loss_val = loss(out.view(-1, model.target_vocab_size), tgt.view(-1))\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    wandb.log({\"loss\": loss_val.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(src, tgt)\n",
    "max_val, max_id = out.max(dim=-1)\n",
    "torch.isclose(max_id, tgt).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations = []\n",
    "for before_p, after_p in zip(init_model.parameters(), model.parameters()):\n",
    "    deviations.append(torch.norm(before_p - after_p).item())\n",
    "\n",
    "plt.hist(deviations)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on WMT14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from itertools import cycle\n",
    "\n",
    "import wandb\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum sequence length\n",
    "MAX_SEQ_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 uses BPE\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target(labels, attention_mask, decoder_start_token_id):\n",
    "    \"\"\"\n",
    "    Prepare decoder target by shifting to the right and adding the start token.\n",
    "    \"\"\"\n",
    "\n",
    "    shifted_labels = labels.new_zeros(labels.shape)\n",
    "    shifted_labels[..., 1:] = labels[..., :-1].clone()\n",
    "    shifted_labels[..., 0] = decoder_start_token_id\n",
    "\n",
    "    shifted_attn_mask = attention_mask.new_zeros(attention_mask.shape)\n",
    "    shifted_attn_mask[..., 1:] = attention_mask[..., :-1].clone()\n",
    "    shifted_attn_mask[..., 0] = 1\n",
    "\n",
    "    return shifted_labels, shifted_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    src_texts = [e[\"de\"] for e in examples[\"translation\"]]\n",
    "    tgt_texts = [e[\"en\"] for e in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        src_texts,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        tgt_texts,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    res = {}\n",
    "    res[\"input_ids\"] = model_inputs[\"input_ids\"]\n",
    "    res[\"attention_mask\"] = ~model_inputs[\"attention_mask\"].bool()\n",
    "\n",
    "    labels, attn_mask = prepare_target(\n",
    "        labels[\"input_ids\"], labels[\"attention_mask\"], tokenizer.pad_token_id\n",
    "    )\n",
    "    res[\"labels\"] = labels\n",
    "    res[\"labels_attention_mask\"] = ~attn_mask.bool()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size, map_batch_size: int = 500):\n",
    "\n",
    "    def _get_dataloader_from_ds(ds):\n",
    "        # TODO: batchsize\n",
    "        # ds = ds.map(encode, batched=True, batch_size=map_batch_size, remove_columns=[\"translation\"])\n",
    "        ds = ds.map(encode, batched=True, batch_size=map_batch_size)\n",
    "        ds = ds.with_format(type=\"torch\")\n",
    "        dl = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "        return dl\n",
    "\n",
    "    # TODO: dataset sizes (take)\n",
    "    # streaming to avoid downloading the whole dataset\n",
    "    train_ds = load_dataset(\"wmt14\", \"de-en\", split=\"train\", streaming=\"True\")\n",
    "    validation_ds = load_dataset(\"wmt14\", \"de-en\", split=\"validation\", streaming=\"True\").take(100)\n",
    "    test_ds = load_dataset(\"wmt14\", \"de-en\", split=\"test\", streaming=\"True\")\n",
    "\n",
    "    train_dl = _get_dataloader_from_ds(train_ds)\n",
    "    validation_dl = _get_dataloader_from_ds(validation_ds)\n",
    "    test_dl = _get_dataloader_from_ds(test_ds)\n",
    "\n",
    "    return train_dl, validation_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tokens(input_ids, model, tokenizer, trim=True):\n",
    "    \"\"\"\n",
    "    Translate tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    if input_ids.dim() == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "    input_ids = input_ids.to(DEVICE)\n",
    "\n",
    "    if trim:\n",
    "        # remove trailing eos tokens (if any)\n",
    "        for last_id in range(input_ids.shape[1] - 1, -1, -1):\n",
    "            if input_ids[0, last_id] != tokenizer.eos_token_id:\n",
    "                break\n",
    "        last_id += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            min_length=2,\n",
    "            max_length=100,\n",
    "        ).squeeze().detach().cpu()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(source, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Translate a text.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(\n",
    "        source,\n",
    "        truncation=True,\n",
    "        max_length=model.max_len,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"]\n",
    "    input_ids = input_ids.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = (\n",
    "            model.generate(\n",
    "                input_ids,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                min_length=2,\n",
    "                max_length=100,\n",
    "            )\n",
    "            .squeeze()\n",
    "            .detach()\n",
    "            .cpu()\n",
    "        )\n",
    "\n",
    "    out = tokenizer.decode(out, skip_special_tokens=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataloader and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    source_vocab_size=tokenizer.vocab_size,\n",
    "    target_vocab_size=tokenizer.vocab_size,\n",
    "    d_model=32,\n",
    "    n_head=8,\n",
    "    d_feedforward=64,\n",
    "    max_len=MAX_SEQ_LENGTH,\n",
    "    max_time_step=10,\n",
    "    halting_thresh=0.8,\n",
    "\n",
    "    batch_size=4,\n",
    "    label_smoothing=0.1,\n",
    "    learning_rate=2e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, validation_dataloader, test_dataloader = get_dataloaders(\n",
    "    config[\"batch_size\"], map_batch_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Allerdings hält das Brennan Center letzteres für einen Mythos, indem es bekräftigt, dass der Wahlbetrug in den USA seltener ist als die Anzahl der vom Blitzschlag getöteten Menschen.',\n",
       " 'However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_sample = next(iter(validation_dataloader))\n",
    "demo_source_txt = demo_sample[\"translation\"][\"de\"][2]\n",
    "demo_target_txt = demo_sample[\"translation\"][\"en\"][2]\n",
    "demo_source_txt, demo_target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UniversalTransformer(\n",
    "    source_vocab_size=config[\"source_vocab_size\"],\n",
    "    target_vocab_size=config[\"target_vocab_size\"],\n",
    "    d_model=config[\"d_model\"],\n",
    "    n_head=config[\"n_head\"],\n",
    "    d_feedforward=config[\"d_feedforward\"],\n",
    "    max_len=config[\"max_len\"],\n",
    "    max_time_step=config[\"max_time_step\"],\n",
    "    halting_thresh=config[\"halting_thresh\"],\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(label_smoothing=config[\"label_smoothing\"]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miibrahimli\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/imran/Desktop/studies/suse_22/neural_networks/universal_transformers/notebooks/wandb/run-20220518_233011-12vfty1o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iibrahimli/universal_transformer_wmt14_test/runs/12vfty1o\" target=\"_blank\">lively-rain-2</a></strong> to <a href=\"https://wandb.ai/iibrahimli/universal_transformer_wmt14_test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"universal_transformer_wmt14_test\", config=config)\n",
    "wandb.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in cycle(enumerate(train_dataloader)):\n",
    "    source = batch[\"input_ids\"]\n",
    "    target = batch[\"labels\"]\n",
    "    source_padding_mask = batch[\"attention_mask\"]\n",
    "    target_padding_mask = batch[\"labels_attention_mask\"]\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(\n",
    "        source,\n",
    "        target,\n",
    "        source_padding_mask=source_padding_mask,\n",
    "        target_padding_mask=target_padding_mask,\n",
    "    )\n",
    "    tr_loss = loss(out.view(-1, model.target_vocab_size), target.view(-1))\n",
    "    tr_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    wandb.log({\"loss\": tr_loss.item()})\n",
    "    logging.info(f\"[{i}] tr_loss: {tr_loss.detach().item():.4f}\")\n",
    "\n",
    "    # validate\n",
    "    if i % 2 == 0:\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        bleu = load_metric(\"bleu\")\n",
    "        for batch in validation_dataloader:\n",
    "            source = batch[\"input_ids\"]\n",
    "            target = batch[\"labels\"]\n",
    "            source_padding_mask = batch[\"attention_mask\"]\n",
    "            target_padding_mask = batch[\"labels_attention_mask\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(\n",
    "                    source,\n",
    "                    target,\n",
    "                    source_padding_mask=source_padding_mask,\n",
    "                    target_padding_mask=target_padding_mask,\n",
    "                )\n",
    "                val_loss = loss(out.view(-1, model.target_vocab_size), target.view(-1))\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                # compute BLEU\n",
    "                source_texts = batch[\"translation\"][\"de\"]\n",
    "                target_texts = batch[\"translation\"][\"en\"]\n",
    "                for src_txt, tgt_txt in zip(source_texts, target_texts):\n",
    "                    translated = translate_text(src_txt, model, tokenizer)\n",
    "                    if len(translated) == 0:\n",
    "                        # to prevent division by zero in BLEU with empty string\n",
    "                        translated = \"0\"\n",
    "                    bleu.add(predictions=translated.split(), references=[tgt_txt.split()])\n",
    "\n",
    "        mean_val_loss = torch.mean(torch.tensor(val_losses)).item()\n",
    "        bleu_score = bleu.compute()[\"bleu\"]\n",
    "        wandb.log({\"val_loss\": mean_val_loss, \"bleu\": bleu_score}, step=i)\n",
    "        logging.info(\n",
    "            f\"[{i}] tr_loss: {tr_loss.detach().item():.4f}  val_loss: {mean_val_loss:.4f}  val_bleu: {bleu_score:.4f}\"\n",
    "        )\n",
    "        logging.info(f\"DE: {demo_source_txt}\")\n",
    "        logging.info(f\"EN: {demo_target_txt}\")\n",
    "        logging.info(f\"output: {translate_text(demo_source_txt, model, tokenizer)}\")\n",
    "        logging.info(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68452a28a195af56f6fef09a39bddd84c9851e5010b5d65c56472b3e191520a0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dl_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
